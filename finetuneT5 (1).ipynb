{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Js50Ox7mIFsI"
      },
      "outputs": [],
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import shutil\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Add special tokens\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': ['<start>', '<end>']})\n",
        "\n",
        "# Read additional data for continued training\n",
        "additional_dataset_path = \"/content/drive/MyDrive/content.txt\"\n",
        "\n",
        "# Create a TextDataset from the additional data\n",
        "additional_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=additional_dataset_path,\n",
        "    block_size=128,\n",
        "    overwrite_cache=True,\n",
        ")\n",
        "\n",
        "# Create a data collator for language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "# Fine-tuning configuration\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/t5-fine-tuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,  # Adjust as needed\n",
        "    per_device_train_batch_size=8,\n",
        "    save_steps=2_500,\n",
        "    save_total_limit=3,\n",
        "    logging_steps=500,\n",
        "    learning_rate=2e-5,\n",
        ")\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=additional_dataset,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"/content/drive/MyDrive/t5-fine-tuned\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/t5-fine-tuned\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Load fine-tuned model and tokenizer\n",
        "fine_tuned_model_path = \"/content/drive/MyDrive/t5-fine-tuned\"\n",
        "model = T5ForConditionalGeneration.from_pretrained(fine_tuned_model_path)\n",
        "tokenizer = T5Tokenizer.from_pretrained(fine_tuned_model_path)\n",
        "\n",
        "# Set generation parameters\n",
        "max_length = 50  # Maximum number of tokens in each generated sequence\n",
        "num_return_sequences = 1  # Number of independent sequences to generate\n",
        "\n",
        "# Start interactive loop\n",
        "while True:\n",
        "    # Get user input\n",
        "    prompt_text = input(\"You: \")\n",
        "\n",
        "    # Tokenize prompt text\n",
        "    input_ids = tokenizer.encode(prompt_text, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate text\n",
        "    output_sequences = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        early_stopping=True  # Stop generation when prediction finishes\n",
        "    )\n",
        "\n",
        "    # Decode generated sequences\n",
        "    generated_texts = [tokenizer.decode(sequence, skip_special_tokens=True) for sequence in output_sequences]\n",
        "\n",
        "    # Print generated text\n",
        "    for i, text in enumerate(generated_texts):\n",
        "        print(f\"Model: {text}\")\n"
      ]
    }
  ]
}